{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-27T12:00:04.754857Z","iopub.execute_input":"2024-04-27T12:00:04.755739Z","iopub.status.idle":"2024-04-27T12:00:04.782140Z","shell.execute_reply.started":"2024-04-27T12:00:04.755688Z","shell.execute_reply":"2024-04-27T12:00:04.781297Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Input\nfrom tensorflow.keras.utils import to_categorical\n\n# Generating data 1000 samples, 10 classes\nnum_classes = 10\ninput_shape = (28, 28)  # MNIST image size\nx_train = np.random.random((1000, 28, 28))\ny_train = np.random.randint(num_classes, size=(1000,))\ny_train_cat = to_categorical(y_train, num_classes)\n\nx_test = np.random.random((200, 28, 28))\ny_test = np.random.randint(num_classes, size=(200,))\ny_test_cat = to_categorical(y_test, num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:49:14.728537Z","iopub.execute_input":"2024-04-27T15:49:14.730180Z","iopub.status.idle":"2024-04-27T15:49:14.751784Z","shell.execute_reply.started":"2024-04-27T15:49:14.730123Z","shell.execute_reply":"2024-04-27T15:49:14.750633Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# define a simple model function\ndef create_model(input_shape, num_classes):\n    inputs = Input(shape=input_shape)\n    x = Flatten()(inputs)\n    x = Dense(128, activation='relu')(x)\n    outputs = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# initialize main and shadow models\nmain_model = create_model(input_shape, num_classes)\nshadow_model = create_model(input_shape, num_classes)\n\n# x_trusted and y_trusted are trusted datasets for shadow model training\nx_trusted = np.random.random((100, 28, 28))\ny_trusted = np.random.randint(num_classes, size=(100,))\ny_trusted_cat = to_categorical(y_trusted, num_classes)\n\nshadow_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nshadow_model.fit(x_trusted, y_trusted_cat, epochs=5, verbose=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:52:21.264881Z","iopub.execute_input":"2024-04-27T15:52:21.265388Z","iopub.status.idle":"2024-04-27T15:52:22.567118Z","shell.execute_reply.started":"2024-04-27T15:52:21.265346Z","shell.execute_reply":"2024-04-27T15:52:22.565690Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0674 - loss: 2.5080  \nEpoch 2/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2040 - loss: 2.2072 \nEpoch 3/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2646 - loss: 2.1059 \nEpoch 4/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3337 - loss: 1.9825 \nEpoch 5/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3737 - loss: 1.8886 \n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7fdeb2bcbc40>"},"metadata":{}}]},{"cell_type":"code","source":"\n# Define the function for custom training step\ndef train_step(model, shadow_model, x_batch, y_batch, lambda_val):\n    with tf.GradientTape() as tape:\n        y_pred = model(x_batch, training=True)\n        primary_loss = tf.keras.losses.categorical_crossentropy(y_batch, y_pred)\n\n        # Consistency check\n        y_pred_shadow = shadow_model(x_batch, training=False)\n        consistency_loss = tf.reduce_mean(tf.square(y_pred - y_pred_shadow))\n\n        total_loss = primary_loss + lambda_val * consistency_loss  # lambda_val is a weighting factor\n\n    gradients = tape.gradient(total_loss, model.trainable_variables)\n    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return total_loss\n\n# Training parameters\nlambda_val = 0.1  # weighting factor\nepochs = 5 \nbatch_size = 32\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_cat)).batch(batch_size)\n\n# custom training step\nfor epoch in range(epochs):\n    for x_batch, y_batch in train_dataset:\n        loss = train_step(main_model, shadow_model, x_batch, y_batch, lambda_val)\n        print(f'Epoch {epoch}, Loss: {loss.numpy()}')\n\n# Evaluate the model\ntest_loss, test_accuracy = main_model.evaluate(x_test, y_test_cat, verbose=0)\nprint(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:00:16.975814Z","iopub.execute_input":"2024-04-27T16:00:16.976247Z","iopub.status.idle":"2024-04-27T16:00:22.694170Z","shell.execute_reply.started":"2024-04-27T16:00:16.976218Z","shell.execute_reply":"2024-04-27T16:00:22.693116Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 0, Loss: [2.3999588 2.0230422 1.2611344 1.7487859 1.6664709 2.4288967 2.0872529\n 1.8408042 1.0994653 2.343625  2.0818322 2.597683  1.7585223 2.182845\n 1.5057302 2.2756097 1.6707774 1.8539776 1.8968621 1.3249266 2.0745494\n 2.0024781 2.1023252 1.4214364 1.0297443 0.8521562 1.5630612 1.5590552\n 1.4611131 1.7211071 1.2645762 1.4769312]\nEpoch 0, Loss: [1.8095186 1.7015901 2.4231458 1.6504366 1.2753625 1.1910536 1.5481203\n 2.0046003 2.2022493 1.8762941 1.1247044 1.388392  2.4189384 1.8974433\n 2.1772614 1.8999145 1.9514008 2.5698977 1.7403939 1.7142639 1.4838998\n 0.7894838 2.339999  2.1664047 1.5382621 0.8974124 1.5209126 1.0320987\n 2.8242462 2.1791985 1.3795704 1.4880178]\nEpoch 0, Loss: [0.9116088 2.5705645 2.1395366 1.3798572 2.1067417 1.6916766 2.2646492\n 1.3636851 1.7936976 2.1983926 1.4844986 1.8572812 2.645198  1.5008848\n 1.7906036 1.6811056 1.6865712 1.9268713 1.5048214 0.6047311 2.0238924\n 1.7338092 1.5781853 2.1047246 1.834081  1.7861402 1.8427148 2.2177413\n 1.9245641 1.3371065 2.3151407 1.6033839]\nEpoch 0, Loss: [1.8280913 1.9027225 1.3259363 1.602445  1.6822504 1.0342913 1.0374187\n 1.4479455 1.5319611 1.6670519 2.0033817 1.7970489 1.2893416 1.4497033\n 1.3870063 1.0670353 2.1743443 1.934301  1.4095389 1.4737018 1.8675307\n 1.4899987 0.9430976 1.730396  1.6815277 2.1545703 1.7492704 1.5226512\n 2.1991043 2.118012  1.6521425 1.1881281]\nEpoch 0, Loss: [1.0274397  1.5662613  1.0633435  1.3103017  2.1874437  2.273496\n 0.88785785 2.058114   1.7343802  1.7357011  1.750005   1.9652276\n 1.4729033  2.1114452  1.6349736  1.8310666  1.5110457  1.3122025\n 1.2473774  1.8348875  1.4060444  3.0381992  1.849855   2.4375014\n 1.5047987  1.5898198  1.1820215  1.7108939  1.9434876  1.9856856\n 1.7438859  1.2054443 ]\nEpoch 0, Loss: [1.5543952  1.5178536  0.9616575  2.7412524  1.3900279  1.4054931\n 1.9547042  1.3121825  1.9458649  1.9286989  2.1040902  2.2766035\n 1.5286616  2.0286868  1.1493812  2.5669737  1.8562317  2.3852677\n 1.3711904  1.9488575  2.4192967  1.575557   2.443185   1.7529169\n 1.6555423  1.9537424  0.83047354 1.3198903  2.0004156  1.3511014\n 1.1515846  2.0133464 ]\nEpoch 0, Loss: [1.7884374 1.6203048 1.165317  1.780635  2.0358884 1.9354844 1.3446178\n 1.6741234 2.8540864 1.1428081 1.5009053 2.2506523 1.785944  1.3664383\n 1.6830509 2.6960444 2.0137262 2.0295112 1.8108778 2.339189  1.7479789\n 1.9337933 2.121594  2.2273645 2.3886633 1.5855175 1.6180154 2.153647\n 1.9085667 2.0783162 1.6367227 2.217848 ]\nEpoch 0, Loss: [1.3935683 1.6663785 1.4938471 1.5443915 1.48608   1.2339008 1.6110476\n 1.5069817 1.4317297 1.4738023 1.3019649 1.6991309 1.8140303 1.9423188\n 1.468634  1.8943292 1.0107455 1.8509    2.0438225 1.8005375 2.030252\n 1.0404431 1.9713625 1.2352475 2.6235132 1.5936528 1.4071909 2.2189126\n 1.5091373 2.3608105 1.7650043 1.824006 ]\nEpoch 0, Loss: [2.2355425 1.6363084 1.6252398 1.9036305 1.5783572 2.6075718 2.3067129\n 2.4646626 1.3097605 1.2561738 2.4686506 1.3728358 2.6307025 1.400817\n 1.123137  1.5060488 2.0026627 1.4118779 1.6846175 1.5163972 1.6669222\n 1.5816562 1.0808198 1.1478927 1.5432522 1.619387  2.174728  1.9370334\n 1.5243298 1.973868  1.5075138 2.0342078]\nEpoch 0, Loss: [2.4613123 2.4269245 2.306468  3.144958  1.4575253 1.3609666 2.2938507\n 2.052868  1.2479432 1.8286779 1.2095599 1.3547258 1.3752933 1.2289771\n 1.7697585 2.0179431 1.3055139 1.4546473 2.1375601 1.4332792 1.6740475\n 1.6917944 1.4314473 1.6997857 1.5753093 1.4374341 1.4034983 1.6166903\n 1.4638656 2.3206139 2.4755154 1.7832445]\nEpoch 0, Loss: [2.162747  1.0578214 1.9051893 1.5183746 1.6922568 2.3286097 1.0597821\n 1.9219285 2.868559  1.0971217 3.0395296 1.6914629 2.3696332 1.7798223\n 1.9386171 1.5132916 1.8128006 1.553059  1.632377  1.3028171 1.5044606\n 1.7019807 3.1266694 2.408791  1.4593296 1.736974  1.898295  1.1226805\n 2.058855  2.12026   1.0522424 1.8083007]\nEpoch 0, Loss: [1.3096762  1.793018   2.2543235  1.4799446  2.3507297  1.97276\n 1.193598   1.4404742  1.8441675  2.561041   1.3544779  1.4772732\n 1.9315712  0.80966336 1.3858777  2.4544766  2.139586   1.167777\n 0.96453243 2.2744398  1.8983073  1.4924412  1.9597557  1.5970528\n 1.7063192  1.5236204  1.7218266  2.3021538  2.6282208  2.2040038\n 2.2238066  2.5302525 ]\nEpoch 0, Loss: [1.1339025  1.5834986  2.25649    2.079844   1.3087971  1.0128562\n 2.7908776  1.6144953  1.5323281  1.8859874  2.3319387  1.8600028\n 1.421567   1.8468362  1.9584583  1.0495387  1.368191   1.6279339\n 2.3342276  1.6625727  1.0151892  1.4825978  1.7944862  1.8084421\n 1.8495219  1.2400979  1.6628451  0.9805758  1.4480331  0.90485483\n 1.53356    1.0956054 ]\nEpoch 0, Loss: [2.0479107 2.0573556 1.8651148 1.6806766 2.132582  1.8377398 2.0290492\n 2.37054   1.4183439 1.8199333 1.56984   1.8411164 1.7309052 1.3102857\n 2.5416248 1.0525796 1.9227612 2.1595428 1.719718  1.8073369 1.5639188\n 1.5638154 2.5486662 2.9191515 2.3204992 1.1566786 1.9514374 0.5879188\n 2.2178733 1.7958151 1.2650424 1.4207813]\nEpoch 0, Loss: [1.9519424  0.83028555 1.6433463  1.7504352  1.0712415  0.72861737\n 2.1353166  1.8216108  2.428427   1.2982343  1.0400058  2.3224883\n 2.0834467  0.9816119  1.0963     2.0017784  2.635514   2.3984787\n 2.346782   1.1719843  1.6449645  2.0155978  2.1683924  1.2329628\n 1.7562903  1.6828836  1.9662038  2.2140682  2.4927852  1.2156978\n 2.1761048  2.4870598 ]\nEpoch 0, Loss: [2.3457582 1.6851845 2.4742568 1.8745627 2.2445955 1.518453  1.7027973\n 1.7147515 1.7131613 2.00969   1.3619596 2.831754  3.3128748 1.8337691\n 1.8395942 2.491961  1.295686  1.5579567 1.5542771 1.7854145 1.4073315\n 2.4124758 2.1861231 2.0735686 1.4522336 2.0761406 1.6376891 1.4934537\n 1.229399  1.7035903 1.5065886 1.2860556]\nEpoch 0, Loss: [2.7108624  1.5142835  1.6236565  2.128773   1.256582   1.737041\n 1.5192131  1.8686445  2.1014807  1.892389   1.919327   2.0320625\n 1.7848399  1.8470134  1.8952503  1.5125394  1.2136928  1.6245527\n 1.7475036  1.9359347  1.8753127  1.9044647  2.0493963  2.7606814\n 1.9552422  0.98287004 1.386494   1.7363012  1.8053888  1.4584267\n 0.7877763  0.74959564]\nEpoch 0, Loss: [1.8132056  2.2931383  2.1017244  1.4174613  2.0483525  1.2949381\n 1.2048514  2.4809136  1.4269881  0.99132025 2.3422794  1.2072237\n 1.5027573  1.802226   1.5311469  1.6112791  1.6570965  1.6472031\n 2.0333965  2.1781204  2.6793764  2.0854938  1.5505687  1.4699948\n 2.5186765  1.563319   1.0145242  1.459616   1.8851447  1.2114135\n 1.2067014  1.408625  ]\nEpoch 0, Loss: [1.8431958 1.7177395 1.4821291 1.5788572 2.0376809 1.6701221 2.1639555\n 1.7518531 2.0275486 2.2525146 1.9930478 1.7820758 1.9402343 1.7745359\n 1.7480259 1.6870971 2.3885877 1.7096831 1.5794476 1.3679209 1.1735657\n 1.984198  1.6904824 1.1982176 1.7131145 2.271446  2.3422687 2.1057773\n 2.0521429 2.05976   2.054447  2.5355992]\nEpoch 0, Loss: [1.0063295  1.6980883  1.8256755  2.3649495  1.1767217  2.4031603\n 1.4508895  1.2380109  1.7508618  1.6520476  1.033625   2.1628153\n 0.96485806 2.0398605  1.5473218  2.029753   1.552391   1.5552127\n 2.2620854  1.9333334  2.2874868  1.264469   2.1271122  1.0082338\n 0.95659965 1.888918   2.0322278  1.3888196  1.5202411  2.355827\n 2.150576   1.6133032 ]\nEpoch 0, Loss: [1.5036719 1.5157166 2.1343348 2.1324914 2.0518515 1.6806071 2.536407\n 1.5108482 1.1956359 2.2953131 2.4841294 1.8569099 2.4070792 1.6155132\n 1.3604434 1.4321194 2.4023716 1.57408   2.1378038 1.706749  1.8873616\n 1.8383273 1.7376364 1.1970949 1.8295735 1.078349  1.7426778 1.1500927\n 1.0002154 1.2300118 1.8429503 0.8933626]\nEpoch 0, Loss: [1.2385815 2.1083362 1.3945141 1.8222871 1.0319102 1.9271231 1.4709053\n 1.7082392 1.3761164 2.6500163 1.8630438 1.7745826 1.5717994 1.3238034\n 1.2583488 1.6127411 1.4987273 1.6423361 1.2754415 2.3656154 2.1156993\n 2.2918403 2.2187133 2.0366323 1.0828762 2.0407262 1.6280512 2.5658944\n 2.0376155 1.834816  1.2853324 2.1872647]\nEpoch 0, Loss: [1.8005017  1.865395   1.5021684  0.59971786 1.0489428  1.5378643\n 1.4491166  2.1410563  1.6380343  1.5959662  1.1328179  2.0441005\n 1.6503011  2.8188553  2.3502364  1.7483721  2.163108   1.5175214\n 1.8296847  2.5196934  1.9363754  1.0227135  0.87667304 2.799572\n 1.9379766  2.4644394  1.4957718  1.429179   1.1482214  1.222035\n 1.6258302  2.289139  ]\nEpoch 0, Loss: [2.9046683 1.6540935 3.1187506 1.8947151 1.4913609 1.5376875 1.4890096\n 1.2683758 2.5713034 1.5953629 1.185695  1.1391672 3.086677  1.5215166\n 1.3000361 2.3196712 1.957796  1.7256901 1.0663849 1.8431973 1.2594165\n 2.930488  1.4885844 2.3808506 0.844719  1.3871673 0.922268  1.92295\n 1.7385652 1.8433511 1.9630497 1.5423523]\nEpoch 0, Loss: [1.3473599  2.241593   1.6000047  2.5054355  1.5868677  1.773394\n 1.6017238  1.8929738  0.69057846 1.8509395  1.9860553  0.9632915\n 1.291206   2.3358064  1.8303115  1.8456976  1.8686539  1.4775405\n 1.1719129  1.427837   1.8360326  1.8536295  1.6742005  1.5861324\n 1.6592727  2.7264962  1.4874338  2.8833098  1.9058943  2.1062248\n 1.5677632  1.6829048 ]\nEpoch 0, Loss: [1.6610641 1.9253879 3.1002953 1.467609  1.9040822 1.0508993 1.4148043\n 1.0172179 2.395219  1.8378541 1.5632228 1.6316104 2.1821802 1.9373404\n 1.7887186 1.6609339 1.6888169 1.7938061 2.5244012 1.3198501 1.3827652\n 2.0801823 2.5330687 1.0071137 1.9865823 1.7427255 2.7354522 1.8398892\n 2.6471844 2.543564  1.2527299 2.7999043]\nEpoch 0, Loss: [1.840156   1.8396668  1.6135087  1.7442555  1.4586369  1.1901559\n 1.7374439  1.4364166  1.5362043  1.2189687  1.326751   1.6076254\n 1.9232086  0.88640577 1.6656519  2.1672344  2.0703225  1.8090122\n 1.9883703  1.5093606  1.1672351  1.0789381  2.4382017  1.4099344\n 1.603478   1.6664575  1.6205456  1.7217531  1.7606813  1.0771551\n 2.01017    2.0302148 ]\nEpoch 0, Loss: [1.8684735 1.0761366 2.0031629 1.6521261 1.4295002 1.0301687 0.9540224\n 1.3783196 2.617373  1.4834237 1.2073468 1.476212  1.652097  2.1734698\n 2.1660702 1.6217874 2.147603  2.4004986 1.524581  1.6777686 0.9010871\n 1.1233886 2.1865456 1.7704512 1.8731863 2.0697024 2.0913174 1.3720222\n 1.7335422 1.9413985 1.9414688 1.4446754]\nEpoch 0, Loss: [2.2833076  1.9847068  1.4065691  1.6584697  2.3081574  0.93519735\n 1.5323784  1.347032   1.6175611  2.2301185  1.2478443  1.2233511\n 1.0145109  1.6721523  1.788352   1.695398   1.3811297  0.8029036\n 2.5216327  1.4805108  2.1283288  2.0243773  1.9630486  2.1185322\n 2.7718542  2.0121374  1.8127246  1.6328511  2.5389974  1.8130617\n 1.682043   1.3643239 ]\nEpoch 0, Loss: [2.0110414  1.4657292  1.2097933  1.4974455  1.2441958  1.0388013\n 1.6930295  1.8434877  1.100927   1.6689733  2.4890707  1.402186\n 1.8531495  1.7436076  0.8296791  1.1495172  1.7028391  1.9301095\n 1.0178372  2.4712818  1.885044   1.2902801  1.7887949  0.82287484\n 2.3686771  1.7363553  2.566192   1.7200727  2.247744   1.4360551\n 2.1140602  1.2683502 ]\nEpoch 0, Loss: [1.2512839  2.7784142  2.568934   1.6642927  1.2938845  2.130117\n 1.897696   1.266687   1.9357665  1.2615671  1.8217139  1.6234186\n 2.056389   0.68081164 1.2438452  2.3444467  2.8528838  1.7117462\n 1.7319416  2.160089   2.5994585  1.0512779  1.498523   2.5325122\n 1.5329345  2.3495758  1.4219579  2.2238839  1.1140537  1.3141724\n 2.2265756  1.9629251 ]\nEpoch 0, Loss: [2.1209335 1.2145973 1.0437455 2.3929214 2.1789892 1.3201458 0.9365081\n 2.1575284]\nEpoch 1, Loss: [2.412466   1.8566793  1.1004517  1.6807592  1.5136902  2.3501146\n 1.9807038  1.7699962  0.96946335 2.3059208  2.0164545  2.5536878\n 1.8019558  2.0897057  1.3924836  2.1574295  1.6268774  1.8093798\n 1.8541695  1.2510059  2.0356524  1.9372884  1.9865167  1.4161452\n 0.92351925 0.80724627 1.479591   1.4169489  1.2942301  1.6571665\n 1.2638186  1.3357533 ]\nEpoch 1, Loss: [1.7528199  1.5112916  2.4217308  1.4769405  1.2902842  1.0388724\n 1.5058135  1.8990107  2.0800066  1.9417137  1.0417503  1.3821989\n 2.4203045  1.8899683  2.165297   1.8257481  1.8522755  2.4741871\n 1.7073407  1.543244   1.50835    0.6793956  2.3803027  2.0422938\n 1.5260069  0.88020146 1.3602157  0.8469588  2.801657   2.2666094\n 1.2444897  1.4249126 ]\nEpoch 1, Loss: [0.82847285 2.449672   2.0634377  1.2254437  2.1184852  1.6517485\n 2.2128031  1.2393607  1.6600358  2.094097   1.3938284  1.8049164\n 2.6945     1.417926   1.8465639  1.5545583  1.5978541  1.9299268\n 1.5524668  0.51371324 2.0259793  1.6561732  1.4122821  1.9698712\n 1.8357646  1.7626213  1.721654   2.2148597  1.9353963  1.3410382\n 2.2684178  1.4869771 ]\nEpoch 1, Loss: [1.8341662  1.8145969  1.2568152  1.4758191  1.5483109  0.9516902\n 0.9134395  1.3267025  1.4872384  1.57127    1.8987594  1.6694034\n 1.2081625  1.3097223  1.2970665  0.9694474  2.067701   1.8191595\n 1.4038953  1.3872259  1.7319858  1.3684304  0.81087524 1.6684356\n 1.6482671  2.0937202  1.7217134  1.4005343  2.1582696  2.1252592\n 1.6957211  1.163709  ]\nEpoch 1, Loss: [0.93415904 1.5013467  0.9374067  1.2814429  2.1176174  2.109492\n 0.7631704  2.1147456  1.608123   1.6616465  1.6182584  1.8813214\n 1.3263558  2.1429787  1.5627382  1.7160013  1.4283712  1.1315178\n 1.2313225  1.7766399  1.4595784  2.9255233  1.7560472  2.3270702\n 1.3651831  1.4746623  1.0763282  1.6743475  1.7799643  1.9423643\n 1.7389202  1.010768  ]\nEpoch 1, Loss: [1.4229212  1.316825   0.7930774  2.7699     1.3193642  1.294595\n 1.8497453  1.152933   1.8257502  1.8388909  2.0512946  2.234556\n 1.4439385  1.9595152  0.99015236 2.4600873  1.8269273  2.251597\n 1.1677635  1.7768636  2.362057   1.5329759  2.379108   1.6014113\n 1.751855   1.760195   0.71500474 1.2715094  1.976997   1.2248119\n 1.0193472  1.9990622 ]\nEpoch 1, Loss: [1.7109334 1.4441458 1.1460159 1.587394  1.868636  1.8650109 1.2865009\n 1.5766166 2.8272893 1.0309209 1.3342066 2.1155922 1.684431  1.3006961\n 1.5263633 2.557549  1.9461199 1.9671153 1.6825285 2.0948575 1.6541038\n 1.8017546 2.030889  2.1852577 2.280813  1.3649435 1.5836177 2.151739\n 1.7070225 2.076334  1.3865608 2.0010846]\nEpoch 1, Loss: [1.2602652  1.456043   1.394618   1.3321067  1.389991   1.0289676\n 1.4803895  1.5612507  1.2563304  1.4418762  1.1781474  1.6294087\n 1.6288143  1.756097   1.3706717  1.7323226  0.97129166 1.7404214\n 1.9372119  1.6184909  1.9819697  0.8600112  1.8628122  1.167948\n 2.5360272  1.4460301  1.3537542  2.1414745  1.5334508  2.2082698\n 1.5767307  1.7815415 ]\nEpoch 1, Loss: [2.2453995  1.3560172  1.4036692  2.0341086  1.26872    2.6372197\n 2.1984634  2.3940234  1.1306561  1.2652266  2.4568005  1.4092541\n 2.59513    1.4147154  1.1066077  1.3959947  1.9998295  1.2931708\n 1.5461172  1.4119405  1.5727708  1.3797957  0.82501566 1.0649163\n 1.5464509  1.3236862  2.130444   1.7709627  1.3158376  1.7941304\n 1.4547151  2.0129552 ]\nEpoch 1, Loss: [2.4517026 2.4590073 2.3871198 3.1452043 1.3058103 1.1374495 2.280398\n 1.9102827 1.1083245 1.5518577 1.1057985 1.3603436 1.3832918 1.2194306\n 1.7630935 1.9879372 1.3077831 1.3946415 2.0301046 1.131798  1.5397648\n 1.5635399 1.1407025 1.6773814 1.5969303 1.3743995 1.3917625 1.4858786\n 1.4008137 2.316775  2.628703  1.7583002]\nEpoch 1, Loss: [2.2772424  0.8447059  1.7585337  1.419608   1.5505722  2.215254\n 0.86768836 1.8643305  2.9008877  0.9763423  3.075476   1.7321274\n 2.287087   1.6164376  1.8996139  1.4328566  1.7857893  1.4358693\n 1.5470766  1.1596845  1.4909618  1.642742   3.1662028  2.4705298\n 1.5048325  1.6087232  1.8175579  1.054239   2.0679853  1.9939187\n 0.83770484 1.7361141 ]\nEpoch 1, Loss: [1.1571529  1.8161516  2.1710427  1.3486642  2.301847   1.9724934\n 0.9722784  1.410972   1.6885623  2.6157308  1.2420115  1.2630458\n 1.7737489  0.64971954 1.2940733  2.52444    2.1238742  1.0161976\n 0.84606236 2.2639225  1.8413899  1.353573   1.8785136  1.5205035\n 1.4948505  1.3925321  1.6696421  2.3068783  2.6561038  2.2099109\n 2.082229   2.4981794 ]\nEpoch 1, Loss: [1.0494252  1.491873   2.254434   2.0855367  1.1405869  0.88775545\n 2.8806093  1.4780087  1.4813744  1.8501986  2.3694859  1.7930198\n 1.253223   1.6877455  1.9253266  0.82814217 1.2031401  1.5184175\n 2.3958123  1.5652833  0.91693854 1.3949565  1.7425569  1.834635\n 1.7379159  1.0760261  1.5874499  0.877216   1.4023154  0.8039994\n 1.4506662  0.96198034]\nEpoch 1, Loss: [1.8975573  2.0087245  1.8549613  1.5579206  2.1195161  1.6949773\n 1.9998004  2.3664608  1.3469579  1.7194815  1.4003516  1.7512391\n 1.6501966  1.1646485  2.2902918  0.9691983  1.886396   2.108572\n 1.5905017  1.6448824  1.5286318  1.4480187  2.5821273  2.7834432\n 2.2914429  1.1215818  1.9016073  0.44747624 2.1324675  1.8470769\n 1.1432427  1.2895865 ]\nEpoch 1, Loss: [1.9165044  0.73353714 1.4881327  1.5320426  0.99593484 0.5391573\n 2.118484   1.7508873  2.4450176  1.2897791  0.9262831  2.2397764\n 2.0020995  0.85555285 1.0310532  2.0039957  2.5997598  2.3455384\n 2.3729894  0.9482686  1.5442314  1.7526468  2.017596   1.1436963\n 1.7777113  1.5246176  1.9606072  2.250118   2.4515402  1.0131646\n 2.1298027  2.2294173 ]\nEpoch 1, Loss: [2.5086827 1.4738835 2.3830698 1.827907  2.0472023 1.4114531 1.6026509\n 1.730009  1.6745018 2.0920744 1.34875   2.684461  3.3662796 1.825341\n 1.7685944 2.3829117 1.2114211 1.5469654 1.5034221 1.623872  1.3990164\n 2.2063103 2.197023  2.0203955 1.3410714 2.0803437 1.6048299 1.4135351\n 1.1116058 1.674389  1.3384016 1.2943265]\nEpoch 1, Loss: [2.7794118 1.4979836 1.6488636 1.968867  1.1663386 1.6616517 1.4480616\n 1.8800198 1.9420012 1.9373254 1.8099977 2.0515604 1.6702125 1.9643201\n 1.9050967 1.3786168 1.0912102 1.7619193 1.7930243 1.9828593 1.7296749\n 1.674143  2.0549753 2.6105332 1.9494015 0.948035  1.2546884 1.4849979\n 1.8124889 1.3294765 0.6938104 0.6412766]\nEpoch 1, Loss: [1.8399304  2.232865   2.0835521  1.4173822  2.0689857  1.184512\n 1.1999913  2.4948008  1.4485966  0.9959354  2.0730367  1.168914\n 1.5343965  1.5024598  1.4347187  1.6114432  1.6660434  1.4938915\n 1.9031669  2.1661808  2.639085   1.924429   1.4090105  1.2582502\n 2.414689   1.4897778  0.87845296 1.3407507  1.530629   1.0865135\n 1.0617021  1.4749719 ]\nEpoch 1, Loss: [1.7002172  1.6278467  1.4915425  1.6738682  1.7933146  1.7505273\n 1.8994982  1.7049218  1.8999662  2.2052042  1.9445564  1.7485069\n 1.8819797  1.7921187  1.6073692  1.6033796  2.4543111  1.4895316\n 1.5852549  1.4101607  0.95594233 1.8623337  1.5591779  1.2108947\n 1.5332431  2.0738716  2.43414    2.1788785  1.9644566  1.9152424\n 1.9893475  2.5696623 ]\nEpoch 1, Loss: [0.9218452  1.6660258  1.9411888  2.4161305  0.99301666 2.0168629\n 1.3568718  1.1876557  1.5815916  1.622529   0.9953888  2.1816158\n 0.81096023 1.8661749  1.406199   2.0785518  1.5919241  1.4948411\n 2.0851686  1.8459083  2.2447712  1.2190564  2.063085   0.8803912\n 0.9691257  1.7947148  1.9087887  1.1670927  1.3546531  2.2707891\n 2.1566312  1.3994068 ]\nEpoch 1, Loss: [1.508941   1.5359708  2.0871787  2.0391338  1.8188164  1.4743322\n 2.6457653  1.4875636  1.1689055  2.4188519  2.2104623  1.8685158\n 2.4996853  1.4946393  1.3328489  1.2879623  2.3632913  1.5575134\n 2.053871   1.6005096  1.7727404  1.7208958  1.4112208  1.1688045\n 1.666486   1.0129293  1.7372031  1.1056433  0.8218463  1.1419867\n 1.5814307  0.84879583]\nEpoch 1, Loss: [1.2685146  2.0367942  1.2627977  1.6847726  0.94229114 1.8747338\n 1.631165   1.5727069  1.3637376  2.3591676  1.7011807  1.6441201\n 1.4155592  1.319085   1.225669   1.4466232  1.3391839  1.5271419\n 1.0414112  2.0104887  1.8567201  1.9721838  2.1534874  1.7868524\n 0.9759592  1.7296513  1.5227345  2.2921724  1.860878   1.8192607\n 1.1746312  1.8461839 ]\nEpoch 1, Loss: [1.5667084  1.7232388  1.4268072  0.45911175 0.8942091  1.5283095\n 1.3103865  1.9918492  1.4698403  1.4955139  1.0203695  2.1511085\n 1.7067723  2.5180852  2.0452085  1.7351515  2.0098662  1.4831733\n 1.5955299  2.5171983  1.8216934  1.0032711  0.71487975 2.4902494\n 1.911365   2.1031513  1.2585483  1.4275252  0.9927734  1.215621\n 1.5740719  2.0875857 ]\nEpoch 1, Loss: [2.5573304  1.576654   2.880934   1.8955941  1.496382   1.4871687\n 1.4397339  1.1696019  2.3325343  1.5883123  1.0886251  1.1314485\n 2.8971002  1.5137047  1.1980834  2.1142988  2.0597825  1.5752704\n 1.0403392  1.9391086  1.1629844  2.7275145  1.3801154  2.175538\n 0.71115524 1.2104449  0.83713406 1.689637   1.5317993  1.8851256\n 1.8509825  1.4689598 ]\nEpoch 1, Loss: [1.3605727 2.2807558 1.6108571 2.2919803 1.5723718 1.7371345 1.5414338\n 1.8435695 0.5557239 1.7741685 1.7014337 0.8555822 1.311623  2.177753\n 1.8150747 1.8681808 1.6781069 1.4770336 1.1306202 1.3552978 1.8453884\n 1.6788127 1.5528272 1.4912999 1.7402065 2.4998229 1.2774763 2.6996956\n 1.829063  1.9207424 1.4460357 1.607166 ]\nEpoch 1, Loss: [1.5801885  1.9005903  2.9102862  1.4053457  1.740888   0.98593915\n 1.3394201  0.934571   2.223574   1.8235558  1.6335673  1.6881878\n 1.9567797  1.8984966  1.6515691  1.503641   1.5922141  1.7155919\n 2.6110394  1.2810935  1.3608629  1.9399731  2.5806682  0.92495596\n 1.8282548  1.6452321  2.4906456  1.8653483  2.4628084  2.3497918\n 1.2249184  2.892136  ]\nEpoch 1, Loss: [1.8427305 1.8347709 1.5830156 1.6819966 1.4176743 1.1155158 1.6997197\n 1.3369913 1.4465388 1.1581293 1.36573   1.5550864 1.9247949 0.7595638\n 1.673343  2.153166  2.0336308 1.7092538 1.7823124 1.350973  1.1144309\n 1.0950763 2.227909  1.2895341 1.4764539 1.7075627 1.4334565 1.6260846\n 1.7942327 1.1326504 1.9808395 2.0465345]\nEpoch 1, Loss: [1.7651544 1.0529703 1.8547751 1.5822172 1.3268107 0.9960258 0.9051439\n 1.2939813 2.4788697 1.4201273 1.0926281 1.4306619 1.434263  2.083852\n 1.9464282 1.5359927 1.993971  2.2862408 1.4216148 1.6156667 0.7848365\n 1.1304799 2.1674795 1.6899005 1.731042  1.9093262 2.0088801 1.2761571\n 1.635099  1.9617666 1.8779789 1.3661014]\nEpoch 1, Loss: [2.1376812  1.8817908  1.3108529  1.6209794  2.189946   0.91547257\n 1.4782455  1.3305609  1.5126226  2.048934   1.1078395  1.1295468\n 0.87551993 1.6614426  1.7714627  1.586836   1.2629542  0.7030225\n 2.4543908  1.436617   2.128686   1.9065075  2.0361748  1.9107652\n 2.6338649  1.9953274  1.7268785  1.4902169  2.3980262  1.7277784\n 1.6297661  1.3349442 ]\nEpoch 1, Loss: [1.9199598  1.4056566  1.1264958  1.4263486  1.2014177  0.9073987\n 1.677926   1.8706875  1.0274059  1.5170572  2.6224816  1.2346615\n 1.8436381  1.5843668  0.75159013 1.0430136  1.6441575  1.7929963\n 0.969689   2.4408438  1.8318348  1.2502036  1.6745178  0.71902925\n 2.192884   1.745487   2.467185   1.5839695  2.1232226  1.2871002\n 1.9871193  1.1890618 ]\nEpoch 1, Loss: [1.2693365 2.7940266 2.474625  1.5932329 1.2105929 2.052907  1.8336287\n 1.2196413 1.8512629 1.2214749 1.8009528 1.62772   1.9460778 0.5760714\n 1.1650923 2.3734484 2.8669813 1.6551147 1.6729699 2.158556  2.6031904\n 1.0640677 1.3912935 2.475377  1.5072589 2.2766123 1.4660062 2.119513\n 0.9901922 1.1945722 2.318883  1.839876 ]\nEpoch 1, Loss: [2.0530372  1.126607   0.903024   2.5672219  2.0900826  1.1779735\n 0.81690747 2.0315547 ]\nEpoch 2, Loss: [2.4332526 1.8172958 0.9451092 1.5081834 1.4111667 2.269363  1.853408\n 1.6461917 0.8123185 2.1851285 1.9529048 2.5117066 1.7535197 1.9998384\n 1.2351661 2.0309734 1.4753575 1.5913165 1.804876  1.1322788 2.0359485\n 1.8731098 1.9173131 1.2682086 0.9651163 0.6630062 1.5660962 1.3689873\n 1.217107  1.5790138 1.2471024 1.2441162]\nEpoch 2, Loss: [1.5955265 1.3954062 2.3594215 1.4067626 1.2279536 1.0933158 1.4685247\n 1.8088851 1.9096632 1.896046  0.9287424 1.2452822 2.4513803 1.8885196\n 2.0826604 1.7635285 1.7049235 2.329209  1.6346722 1.4203188 1.3644165\n 0.5006741 2.4191768 1.982016  1.3665687 0.7194228 1.2638814 0.7855273\n 2.8232772 2.171743  1.1566919 1.2787299]\nEpoch 2, Loss: [0.68734545 2.3100464  1.9428725  1.2585385  1.980797   1.5939196\n 2.133144   1.2912391  1.4978001  1.960438   1.2404779  1.821476\n 2.648762   1.31586    1.8275079  1.49255    1.5071921  1.8890212\n 1.4242996  0.4036491  2.0415154  1.6002305  1.3261523  1.8703097\n 1.7303689  1.7293923  1.5708131  2.0628881  1.9421774  1.2109017\n 2.1651974  1.4745791 ]\nEpoch 2, Loss: [1.6890208  1.6882676  1.2279289  1.4583635  1.3810099  0.9479525\n 0.82361037 1.2236153  1.4750355  1.5737127  1.8433573  1.5431528\n 1.23373    1.2359506  1.2325796  0.90234756 1.9272721  1.6719552\n 1.3637785  1.303041   1.6469874  1.3414633  0.68400615 1.5639318\n 1.6274158  2.0445242  1.6094861  1.2604554  2.0882215  2.0474997\n 1.5922678  0.9743966 ]\nEpoch 2, Loss: [0.8571647  1.4260933  0.84095347 1.1888726  2.059806   1.9752752\n 0.6728524  2.106435   1.5010766  1.5730491  1.5092907  1.7363571\n 1.2638154  2.0189154  1.5063404  1.6492232  1.3133694  1.0275031\n 1.1347408  1.6613445  1.4739776  2.86483    1.6409065  2.250723\n 1.1537653  1.3984301  0.99566483 1.5639328  1.6636908  1.855374\n 1.6265956  0.9237605 ]\nEpoch 2, Loss: [1.2449788  1.1550524  0.7392248  2.717292   1.2365767  1.1388696\n 1.7465761  0.99479175 1.7492336  1.7591641  1.9727768  2.2307723\n 1.3179377  1.9391074  0.8963479  2.2859907  1.7367167  2.1904473\n 1.0975194  1.6762764  2.218359   1.4727143  2.2659407  1.5094129\n 1.8406425  1.6808875  0.6314273  1.1514088  1.9468391  1.1161492\n 0.8608507  1.8832613 ]\nEpoch 2, Loss: [1.6069657 1.3591548 1.1415393 1.529943  1.8168826 1.7861786 1.155782\n 1.5179455 2.7362237 0.8997242 1.2521201 2.1894453 1.5769467 1.126484\n 1.5056735 2.4531243 1.9250406 1.7988102 1.6105996 2.0721548 1.4659792\n 1.7975204 1.9141368 1.9467933 2.2234943 1.3083698 1.3661159 2.013216\n 1.6259384 2.05347   1.2460613 1.9462333]\nEpoch 2, Loss: [1.2297771  1.4584546  1.2987448  1.273682   1.2064861  0.9856711\n 1.4439281  1.650112   1.2694181  1.4421263  1.1176993  1.5191978\n 1.5398899  1.7293123  1.2961278  1.6945763  0.82424295 1.5622588\n 1.9555664  1.5946215  1.8485789  0.74727786 1.8678104  1.0134966\n 2.5708385  1.3707749  1.2433676  1.9724799  1.312859   2.1749437\n 1.4619721  1.8172945 ]\nEpoch 2, Loss: [2.0745609 1.2565669 1.3006607 1.7998376 1.193663  2.4991963 2.1755707\n 2.404553  1.0233511 1.2659751 2.2480812 1.411323  2.6791713 1.4086714\n 0.9419163 1.3901083 1.9019809 1.2390621 1.4863011 1.3063308 1.5627819\n 1.2856448 0.7168445 1.04191   1.5067011 1.2567568 1.9181968 1.8196182\n 1.2507474 1.732093  1.4183455 1.9461566]\nEpoch 2, Loss: [2.3093417  2.24759    2.2693892  2.9874814  1.2526859  1.009871\n 2.1003902  1.7957087  1.0308455  1.4958806  0.97413164 1.3168832\n 1.2613168  1.0348526  1.610736   1.8850728  1.2489444  1.3427279\n 1.9433666  1.1003844  1.4560349  1.4159749  1.0501463  1.6843373\n 1.3733124  1.1934459  1.2263314  1.438935   1.3657752  2.2660723\n 2.482485   1.6868788 ]\nEpoch 2, Loss: [2.3027036  0.84070224 1.5505503  1.2909558  1.4027951  2.1317356\n 0.8195851  1.6657418  2.8184469  0.9000903  2.9366436  1.6805451\n 2.2098773  1.5719801  1.8113347  1.3042665  1.6782547  1.2791766\n 1.4984752  1.0865036  1.3238136  1.6702125  3.2183466  2.4291856\n 1.2888608  1.5498813  1.7189078  1.0112011  1.8960568  1.8764848\n 0.75026995 1.6309518 ]\nEpoch 2, Loss: [1.0509237  1.772303   2.1947582  1.2549742  2.3238366  1.7927285\n 0.84808934 1.3225927  1.5253034  2.4804099  1.1388849  1.1685997\n 1.7082733  0.5841234  1.1375316  2.3589149  1.9033762  0.88974047\n 0.70362526 2.200321   1.7369179  1.2554239  1.7821475  1.3472549\n 1.3546952  1.326131   1.6652693  2.118973   2.4898825  2.1487024\n 2.0336108  2.541689  ]\nEpoch 2, Loss: [0.9414752 1.4131798 2.137334  2.0646737 1.0031408 0.816897  2.7769392\n 1.4088941 1.3927516 1.7986519 2.1900408 1.6777887 1.2367607 1.578264\n 1.7228898 0.7940601 1.056519  1.5034126 2.439845  1.5122535 0.8138722\n 1.2679673 1.6393234 1.6574881 1.6065967 0.8995063 1.5151583 0.775693\n 1.3073897 0.6736299 1.4050304 0.9063705]\nEpoch 2, Loss: [1.9381356  1.9899052  1.6844728  1.5150765  1.9529722  1.5728043\n 2.000341   2.2500622  1.2115873  1.5642437  1.2815322  1.7286098\n 1.5061431  1.0777702  2.3154285  0.84919786 1.7154262  2.1514242\n 1.455525   1.668088   1.4284639  1.3801448  2.652104   2.883654\n 2.3050385  1.0003805  1.7933103  0.38351524 2.1094377  1.8382918\n 1.0956105  1.1678684 ]\nEpoch 2, Loss: [1.8444664  0.6677562  1.3749026  1.4752293  0.84287983 0.427818\n 1.9848623  1.5937217  2.3183274  1.1042683  0.8688581  2.1697214\n 1.9392706  0.7663901  0.90550727 1.8840876  2.6788847  2.3752093\n 2.202032   0.83063203 1.4221337  1.7052017  1.9478242  1.0295222\n 1.6512873  1.5689912  1.8133764  2.239795   2.5009239  0.910911\n 1.9997414  2.2875197 ]\nEpoch 2, Loss: [2.4737883 1.3985736 2.4934018 1.6931158 2.048753  1.2850574 1.5821881\n 1.6600193 1.5554795 1.9456023 1.1588656 2.7667103 3.4062474 1.6999947\n 1.674505  2.4810169 1.1432024 1.406097  1.375252  1.6113827 1.2682225\n 2.306824  2.0695906 1.990556  1.2200454 1.9491686 1.5523878 1.2937692\n 1.0385519 1.523789  1.2430273 1.1494353]\nEpoch 2, Loss: [2.7311237 1.4516306 1.4671617 1.8971484 0.9999857 1.6241713 1.30841\n 1.82895   1.8706341 1.8581036 1.7337346 1.9149909 1.5637106 1.7537512\n 1.811665  1.3064339 1.0017534 1.6608667 1.6768576 1.9265122 1.7513379\n 1.6226901 1.9230034 2.6538618 1.8418549 0.8725297 1.1628791 1.3877076\n 1.7178698 1.2236791 0.6041194 0.6028064]\nEpoch 2, Loss: [1.8080887  2.1141026  2.0344365  1.333985   1.9606524  1.1263579\n 1.1061043  2.4837515  1.3421942  0.8379     2.0563605  1.1026335\n 1.4162073  1.4440157  1.3723828  1.5033182  1.5662353  1.396928\n 1.8363544  2.2046247  2.6691582  1.8791621  1.2353626  1.1005281\n 2.4791     1.3742735  0.73726285 1.2737428  1.5162611  0.9523275\n 0.9825885  1.3570156 ]\nEpoch 2, Loss: [1.4769621  1.6070217  1.3388884  1.5214783  1.7634904  1.6991998\n 1.8241621  1.6931517  1.7533599  2.0698535  1.8942686  1.7674417\n 1.7347172  1.7345603  1.574753   1.4963347  2.4672108  1.4423134\n 1.5142316  1.1420321  0.89993197 1.7837235  1.4733652  1.0969359\n 1.4174016  2.0872414  2.2803972  2.0457373  1.9764646  1.8464872\n 1.8365104  2.5806167 ]\nEpoch 2, Loss: [0.79024374 1.5285921  1.8662906  2.4402318  0.8465351  2.0678735\n 1.1890652  1.1360561  1.5034041  1.5036671  0.8606908  2.0488799\n 0.7390111  1.8983253  1.4194198  2.055161   1.380197   1.4182649\n 2.0920265  1.855837   2.2140992  1.0873383  2.1376379  0.78919894\n 0.8463495  1.8851887  1.8123418  1.0926996  1.1956216  2.1166642\n 2.2100093  1.3657275 ]\nEpoch 2, Loss: [1.3400288  1.3864568  1.8767408  1.9611927  1.6583475  1.419713\n 2.6251757  1.4526365  1.0801239  2.2653625  2.1540964  1.8791584\n 2.310622   1.5080589  1.2179785  1.1057733  2.4536028  1.4339406\n 1.9842598  1.5019326  1.8927803  1.7462926  1.4111032  1.0625206\n 1.6535592  0.9045908  1.6091374  1.0286292  0.6254294  0.9826147\n 1.4339788  0.73108584]\nEpoch 2, Loss: [1.244957   1.916906   1.2617666  1.499099   0.8603416  1.8116591\n 1.41866    1.5879867  1.3708649  2.335734   1.7308645  1.6742018\n 1.293674   1.2125196  1.0481306  1.4382062  1.2835706  1.447873\n 0.97337735 2.0190475  1.780985   1.9672047  2.1281867  1.5800909\n 0.9493995  1.6460909  1.4765178  2.2266092  1.8110408  1.665512\n 1.1646501  1.7706531 ]\nEpoch 2, Loss: [1.593067   1.642691   1.2766832  0.37947312 0.7951263  1.5338064\n 1.2488519  1.7584015  1.427432   1.3686571  0.9145722  2.0847297\n 1.7710066  2.4382155  1.9632653  1.5947616  2.0281055  1.3793167\n 1.4005506  2.6562142  1.6634494  0.97350854 0.6881431  2.4888542\n 1.97075    2.1827438  1.3062158  1.4658202  0.89938605 1.089774\n 1.4281019  2.0298123 ]\nEpoch 2, Loss: [2.5259798  1.5295627  2.860231   1.8159913  1.3976264  1.3897078\n 1.2599564  1.0301434  2.2500765  1.6987927  0.93693167 1.0315446\n 2.942524   1.3137691  1.1377647  2.0288925  1.8184423  1.3810043\n 0.90772927 1.6938035  1.0479258  2.6032     1.2492756  2.1012938\n 0.6797301  1.1890875  0.6838367  1.6590943  1.3830209  1.6305786\n 1.7799281  1.3506333 ]\nEpoch 2, Loss: [1.2051117  2.12779    1.6557971  2.347134   1.6247516  1.7240337\n 1.3847839  1.8272831  0.47364786 1.739853   1.6324947  0.71259975\n 1.3248117  2.1475713  1.7867752  1.7812093  1.5722855  1.4204854\n 1.1169785  1.211035   1.8042357  1.4692417  1.4342021  1.4103087\n 1.7333305  2.306714   1.1055484  2.7333767  1.7085446  1.8155818\n 1.4011837  1.5182569 ]\nEpoch 2, Loss: [1.4873269  1.9097797  2.864987   1.2896304  1.6017255  0.9183089\n 1.2009234  0.7913721  2.1930122  1.7392237  1.5279619  1.5233773\n 1.923437   1.8719809  1.5504454  1.489111   1.4816608  1.5326111\n 2.543036   1.2194893  1.2587651  2.0030882  2.6237392  0.75952464\n 1.7442522  1.4794338  2.5335498  1.8382449  2.387061   2.2442775\n 1.0962523  2.8361087 ]\nEpoch 2, Loss: [1.9246726  1.6825514  1.578303   1.4763242  1.2932563  0.9514956\n 1.583976   1.2043446  1.3802664  0.9894909  1.2186186  1.4102073\n 1.9202164  0.61562085 1.5321457  2.1999748  2.062423   1.5857798\n 1.6948425  1.2821218  0.9446122  0.9459961  2.1641135  1.1899052\n 1.4720554  1.6312673  1.414596   1.4811065  1.6646515  1.0233842\n 1.9294854  2.0984292 ]\nEpoch 2, Loss: [1.7206467 0.8853013 1.8415892 1.5390737 1.124887  0.8078921 0.825069\n 1.125567  2.4508991 1.277537  0.9944014 1.4149959 1.3302375 2.2277496\n 1.8688912 1.3341827 2.0194726 2.352197  1.3164314 1.4584265 0.6206018\n 1.0205982 2.1058917 1.5676734 1.6521193 1.7994668 1.9351438 1.1876746\n 1.6224395 1.9087732 1.8435739 1.2056856]\nEpoch 2, Loss: [2.0575213  1.7932668  1.2380767  1.6021825  2.1512144  0.75483435\n 1.4455414  1.1938602  1.3779958  1.9389211  0.9966869  0.97325844\n 0.8190137  1.4812745  1.6698651  1.5017527  1.1509494  0.5255478\n 2.5041177  1.2676755  2.269477   1.859768   1.9360908  1.7898134\n 2.6500428  2.0579398  1.5977099  1.4872015  2.3533955  1.7173872\n 1.6088322  1.1602511 ]\nEpoch 2, Loss: [1.8981006  1.2696187  0.99426806 1.3421685  1.0624039  0.7797472\n 1.645008   1.7955366  0.8745775  1.4093801  2.6173482  1.2204003\n 1.7756512  1.459708   0.6768221  0.91908276 1.4916328  1.7275258\n 0.8457517  2.4642825  1.7534976  1.1293545  1.5031588  0.6209535\n 2.098483   1.7245495  2.2698996  1.5212277  2.0048456  1.2414501\n 1.9341612  1.031524  ]\nEpoch 2, Loss: [1.1744089  2.6681108  2.3413363  1.4872702  1.104363   2.037293\n 1.8675884  1.1034989  1.8601111  1.1235751  1.6739284  1.5794415\n 1.9632915  0.4937412  1.0308021  2.3653088  2.868884   1.6134857\n 1.6024289  2.0902896  2.5167358  0.9667433  1.4084902  2.4992816\n 1.4215221  2.2383504  1.3628643  2.0132627  0.89353293 1.1422461\n 2.274661   1.7042316 ]\nEpoch 2, Loss: [2.1787014 1.0423568 0.8597617 2.4893901 1.9314388 1.16723   0.7504056\n 2.051559 ]\nEpoch 3, Loss: [2.2140877  1.7018608  0.8177952  1.4432318  1.3082981  2.3397582\n 1.7801143  1.4928961  0.7214768  2.0410159  1.895507   2.3985963\n 1.7681618  1.9526514  1.1779115  2.064492   1.4072254  1.502424\n 1.6473005  1.1968148  1.91334    1.7161394  1.7209312  1.2142775\n 0.83402485 0.61626077 1.4388556  1.3935529  1.20897    1.4003353\n 1.1166494  1.1500759 ]\nEpoch 3, Loss: [1.5060841  1.3033278  2.319457   1.3197991  1.210153   0.91938597\n 1.4838763  1.6955922  1.7999308  1.8663446  0.93067473 1.1765456\n 2.335248   1.7211623  2.0187895  1.5881865  1.6230521  2.3163843\n 1.4759887  1.2402745  1.2812054  0.41147846 2.3529918  1.93478\n 1.3466672  0.65758914 1.1997945  0.68042845 2.719767   2.180606\n 1.1359012  1.2917259 ]\nEpoch 3, Loss: [0.6656734  2.3237472  1.852504   1.066655   1.9471476  1.4471872\n 2.0560994  1.0763572  1.3398935  1.87213    1.2036537  1.6781563\n 2.5813918  1.3265053  1.8865156  1.4785731  1.5520575  1.8000816\n 1.4455267  0.36536583 1.8884876  1.4754491  1.2292681  1.8201021\n 1.6713501  1.5905564  1.4434562  2.0080116  1.851447   1.1681819\n 2.1162605  1.4085022 ]\nEpoch 3, Loss: [1.792784   1.636107   1.1833544  1.2857846  1.2691046  0.8212318\n 0.77908283 1.1788493  1.3432302  1.4232041  1.6889658  1.4497504\n 1.0813081  1.0047338  1.2296693  0.7761382  1.8505251  1.5610963\n 1.2260745  1.2866422  1.4688467  1.1543448  0.59943134 1.464959\n 1.4321327  2.0145564  1.48948    1.1004364  2.0024924  1.9262468\n 1.6202339  1.051014  ]\nEpoch 3, Loss: [0.7955129  1.410152   0.70599294 1.166255   2.0251186  1.7773557\n 0.5635077  2.0799053  1.3283526  1.5854201  1.4117583  1.6325095\n 1.1206591  2.1770446  1.4366534  1.6379726  1.2957315  0.9044268\n 1.0676     1.6290029  1.4166121  2.843959   1.4913771  2.1241403\n 1.0538943  1.2208352  0.8591494  1.5945895  1.4909365  1.8645247\n 1.631454   0.7870263 ]\nEpoch 3, Loss: [1.1844069  0.9436147  0.64188874 2.8380446  1.1338652  0.98461103\n 1.6457916  0.8840195  1.5945059  1.7452252  1.9003903  2.2090225\n 1.261789   1.9845918  0.79269904 2.2172706  1.700961   2.0063396\n 0.8884028  1.4979471  2.182381   1.3358446  2.1919777  1.3659695\n 1.8678695  1.5755749  0.55091286 1.1241792  1.8685945  1.0018544\n 0.7349842  1.8540889 ]\nEpoch 3, Loss: [1.5951712  1.2199103  1.1142471  1.3207991  1.6888041  1.7233645\n 1.1423949  1.3094687  2.6626914  0.80999887 1.0484349  2.2038355\n 1.4530424  1.1686525  1.4297832  2.4202988  1.8375833  1.8185178\n 1.5091854  1.8252759  1.3989837  1.6202555  1.6727024  1.9782858\n 2.0246358  1.2051374  1.3721932  2.0106168  1.537544   1.8790901\n 1.0131031  1.8170555 ]\nEpoch 3, Loss: [1.0900605  1.1786284  1.2436873  1.0664937  1.1835047  0.8249112\n 1.3379227  1.6537765  0.95122224 1.3705726  0.95889074 1.3879718\n 1.3809214  1.4925287  1.2094157  1.6012509  0.883034   1.4118136\n 1.7161458  1.5559338  1.770234   0.54332685 1.7913339  1.0250597\n 2.472844   1.1905378  1.18538    1.8796824  1.4516097  2.0754328\n 1.3240126  1.7927731 ]\nEpoch 3, Loss: [2.0362673  0.97563916 1.1270101  1.9480084  0.96717983 2.45074\n 2.0364711  2.5946722  0.9586483  1.4271088  2.1327116  1.3783858\n 2.7778594  1.4958752  0.93842745 1.1985446  1.7794845  1.1475022\n 1.5265146  1.1412877  1.4518511  1.1673132  0.53360987 1.0608623\n 1.563126   1.0581768  1.8557981  1.683596   1.1739279  1.4663574\n 1.2858115  1.9172804 ]\nEpoch 3, Loss: [2.2757647  2.2216163  2.2847488  2.9867833  1.2472293  0.793524\n 2.1035616  1.8032739  0.99289143 1.4270085  0.8105896  1.4388036\n 1.1906171  1.0936118  1.4866256  1.8618042  1.2550207  1.3798598\n 1.938687   0.9094534  1.4820259  1.3539363  0.80182016 1.6005341\n 1.5321974  1.2338547  1.2273272  1.3408334  1.1768876  2.1372442\n 2.4966502  1.5344619 ]\nEpoch 3, Loss: [2.0607123  0.6712035  1.5445615  1.1492398  1.3842956  2.2266705\n 0.7434479  1.5218629  2.8309257  0.7600481  2.962029   1.8769652\n 2.0175729  1.4093196  1.7854528  1.3058046  1.508009   1.206584\n 1.5820873  0.95363504 1.3373604  1.5724518  3.431325   2.2719069\n 1.2224725  1.4905286  1.8126283  0.84805113 1.7658743  1.8532048\n 0.61398077 1.6410791 ]\nEpoch 3, Loss: [0.9579916  1.7452253  1.9994005  1.13243    2.1355727  1.789424\n 0.70439976 1.3176384  1.447679   2.5805902  0.9592951  0.99378705\n 1.7100664  0.45885456 1.1054095  2.351897   1.8730223  0.80785453\n 0.6639885  2.132942   1.5586127  1.2215291  1.7186207  1.1962849\n 1.2980064  1.089455   1.5657399  2.0445786  2.486824   2.130228\n 1.7288857  2.3374484 ]\nEpoch 3, Loss: [0.8441186  1.466163   2.0002     1.9063125  0.8761132  0.70150065\n 2.7546778  1.1759963  1.4936818  1.8100882  2.107536   1.5745014\n 1.2120856  1.5211631  1.6344033  0.71222806 0.96122813 1.4157695\n 2.4567552  1.4160525  0.72450936 1.207383   1.4911532  1.5830634\n 1.5077444  0.82996553 1.4725896  0.69462895 1.2851657  0.5616896\n 1.287431   0.9441775 ]\nEpoch 3, Loss: [1.5787693  1.8070409  1.6454505  1.3744549  1.8831841  1.4754094\n 1.8078772  2.1556537  1.0549673  1.4603773  1.1330279  1.6955032\n 1.3937211  0.97506446 1.9476293  0.7090355  1.659184   1.8847084\n 1.2775383  1.3324734  1.3590007  1.2241721  2.5358984  2.5321245\n 2.1599548  0.9797364  1.8087666  0.29384026 2.0559409  2.0371046\n 1.0857328  1.0818928 ]\nEpoch 3, Loss: [1.7064075  0.6163732  1.2037959  1.3102063  0.8255516  0.31969288\n 1.9023665  1.5407264  2.2893705  1.2351923  0.9417052  2.0460534\n 1.7390134  0.6690271  0.9031542  1.8260481  2.6122248  2.1942341\n 2.1877458  0.7245824  1.4568679  1.5778738  1.78276    1.013414\n 1.7191039  1.4368296  1.7777178  2.0895665  2.4118137  0.8152607\n 1.9650153  1.9422424 ]\nEpoch 3, Loss: [2.5993283 1.1568413 2.1858432 1.5922867 1.8158787 1.287793  1.4834262\n 1.7858287 1.4464388 1.9621124 1.0822972 2.4617014 3.6747    1.6624222\n 1.5182471 2.2057705 1.1019171 1.3758433 1.3595402 1.5181735 1.2345817\n 2.1281123 2.0278256 1.8631198 1.030537  1.9252185 1.637689  1.2002355\n 0.8605506 1.4422572 1.0857295 1.1062444]\nEpoch 3, Loss: [2.7603257  1.3161875  1.4979624  1.7544361  0.9404631  1.5794853\n 1.2427319  1.6970409  1.6256719  1.8709111  1.6541908  1.8842344\n 1.4062101  1.7948619  1.7976961  1.16797    0.8159623  1.6805916\n 1.6831833  1.9589977  1.496794   1.5251235  1.9023774  2.4402955\n 1.845675   0.7455515  0.9664241  1.262681   1.7047474  1.0927768\n 0.57720536 0.54630196]\nEpoch 3, Loss: [1.816993   2.0739617  1.9721663  1.3001735  1.946218   1.0976977\n 1.0800363  2.4593952  1.3364397  0.726228   1.9885913  0.960025\n 1.4587958  1.3739961  1.2643015  1.4706849  1.5310144  1.1645222\n 1.7444165  2.1278048  2.5833857  1.6372322  1.190645   0.8636407\n 2.247496   1.3142565  0.60552883 1.0997326  1.3762256  0.9171328\n 0.8624151  1.4073408 ]\nEpoch 3, Loss: [1.3570693  1.4368991  1.304613   1.4816996  1.7659103  1.7169309\n 1.802384   1.64038    1.6988837  2.0191617  1.7289026  1.734995\n 1.650215   1.5839251  1.4758643  1.3364754  2.3891516  1.3424453\n 1.5043606  1.2717856  0.76624787 1.7382026  1.4295235  1.1047356\n 1.3175173  2.0301292  2.305761   2.0495265  1.8520652  1.6991838\n 1.7657787  2.5956416 ]\nEpoch 3, Loss: [0.7329096  1.4037349  1.9841857  2.3177543  0.7048958  2.0070794\n 1.1202633  0.96732813 1.4710826  1.4211075  0.83700037 2.0563774\n 0.6470022  1.6816142  1.2460424  2.110227   1.4211566  1.2572485\n 2.0109859  1.8006129  2.141011   1.0576683  1.9738747  0.7282978\n 0.8236677  1.742571   1.6905098  0.965524   1.1464846  2.1519206\n 2.101504   1.2884364 ]\nEpoch 3, Loss: [1.3305234  1.2961289  1.7790976  1.879652   1.5764627  1.3693963\n 2.5387876  1.3706013  1.0202396  2.3939068  2.1030304  1.8727398\n 2.3977292  1.3493899  1.1274246  1.0531707  2.1553605  1.3493047\n 2.007417   1.5114076  1.7027309  1.5893744  1.2931075  0.94649994\n 1.5883071  0.8039771  1.5970141  0.93736506 0.5370665  0.8701966\n 1.2625831  0.6838977 ]\nEpoch 3, Loss: [1.2315943 1.7866907 1.0497489 1.4667649 0.8169842 1.7005539 1.5315564\n 1.4329144 1.3605427 2.2735505 1.6549346 1.5357254 1.2666727 1.1777418\n 0.9722496 1.2624276 1.0927391 1.4277865 0.7671272 1.9415724 1.7671423\n 1.9255158 1.9937822 1.5410014 0.8419977 1.5664573 1.3479179 2.2427385\n 1.5903077 1.616618  0.982137  1.718185 ]\nEpoch 3, Loss: [1.2954438 1.4725904 1.1492183 0.2911757 0.7147896 1.5392923 1.1446561\n 1.6978902 1.1944965 1.1669348 0.7453434 2.1492453 1.749257  2.3503878\n 1.8913733 1.4628015 1.9149877 1.3075099 1.291885  2.5364325 1.6425925\n 0.9472519 0.5570426 2.3847177 1.7157445 2.041754  1.0684704 1.4583347\n 0.8254233 1.0338504 1.3091605 1.9467332]\nEpoch 3, Loss: [2.356164   1.3935479  2.8179007  1.7604418  1.2249631  1.2297381\n 1.1650358  0.904429   2.2360713  1.7429811  0.8149111  0.9737756\n 2.9609838  1.4055499  1.0611461  1.9100267  1.9541627  1.2967043\n 0.8644371  1.8495953  0.895492   2.5906258  1.1485736  2.0718122\n 0.5672191  1.0386676  0.59009314 1.570833   1.3017555  1.7286428\n 1.672038   1.271453  ]\nEpoch 3, Loss: [1.1110673  2.2566364  1.6427859  2.2465994  1.638629   1.5784258\n 1.2415589  1.7600228  0.36801833 1.589858   1.4565716  0.65932286\n 1.3509556  2.0725749  1.717921   1.7224858  1.4715521  1.407181\n 1.0973941  1.0692445  1.6567067  1.3574744  1.4123993  1.2115197\n 1.7576278  2.2416537  0.9707031  2.6718757  1.5837791  1.7185777\n 1.2879527  1.3943369 ]\nEpoch 3, Loss: [1.3563504 1.9732993 2.870037  1.1524264 1.420937  0.8240777 1.0615271\n 0.7510872 2.0094287 1.6318042 1.516009  1.7896259 1.7574002 1.7904869\n 1.4998816 1.3355304 1.4220228 1.5733774 2.5725572 1.1010809 1.1726269\n 1.8782758 2.5869997 0.6479139 1.7010815 1.2939537 2.4435413 1.7697097\n 2.3972976 2.218813  1.0230206 2.8040078]\nEpoch 3, Loss: [1.9330523  1.6422223  1.5937825  1.4068816  1.1834335  0.87907135\n 1.4746901  1.027772   1.2131367  0.954035   1.189916   1.3318855\n 1.936485   0.53709805 1.5511721  2.141591   2.0854146  1.4069221\n 1.5263269  1.2365978  0.8884385  0.9307692  2.0953267  1.0471601\n 1.3775803  1.5851989  1.274147   1.2871691  1.5845032  1.0416809\n 1.8474871  2.0147564 ]\nEpoch 3, Loss: [1.5022023  0.8961801  1.6382136  1.4122367  1.0299081  0.77231205\n 0.7903331  1.0084884  2.2486212  1.2105836  0.8204052  1.4146547\n 1.0579199  2.10099    1.6008183  1.277473   1.8919383  2.3329268\n 1.3247631  1.3850715  0.5361201  1.006987   2.0104916  1.3765957\n 1.455418   1.7496439  1.9418185  1.1109864  1.451221   1.8213199\n 1.7675095  1.2521187 ]\nEpoch 3, Loss: [1.9951204  1.5851626  1.1060481  1.5734717  2.127601   0.7167579\n 1.4053575  1.1949983  1.2878522  1.857336   0.8421503  0.90328056\n 0.7484065  1.4983275  1.6490903  1.3361443  1.0768126  0.46520337\n 2.4186752  1.2063518  2.3245106  1.6721104  1.9985015  1.6546578\n 2.3907251  2.0653152  1.4360526  1.3541957  2.1633468  1.5831077\n 1.4942775  1.1982081 ]\nEpoch 3, Loss: [1.8533334  1.2231684  0.92968833 1.3443798  1.0974038  0.6848964\n 1.568228   1.7577416  0.8471075  1.21889    2.5962262  1.0867114\n 1.6141384  1.3076823  0.61829406 0.9091207  1.2847512  1.5508282\n 0.8600525  2.404069   1.6372876  1.0736473  1.3682008  0.5609689\n 1.9708191  1.6339213  2.242447   1.2860523  1.8619996  1.145451\n 1.7222075  0.9710802 ]\nEpoch 3, Loss: [1.1369078  2.6872926  2.3007271  1.3876464  1.0911698  1.9757603\n 1.7983149  1.1225677  1.7223626  1.0131936  1.6867187  1.6141471\n 1.853611   0.4449019  1.04326    2.4087713  2.9317236  1.5233829\n 1.6478704  2.0829268  2.5031543  0.93281054 1.2942659  2.4016914\n 1.373033   2.2129638  1.2842429  1.9045293  0.81184244 1.0314572\n 2.214821   1.5920013 ]\nEpoch 3, Loss: [2.0939462  0.8893018  0.7604472  2.5193956  1.7751449  1.0436953\n 0.65234894 1.8996602 ]\nEpoch 4, Loss: [2.1456716  1.607124   0.73110515 1.3057865  1.2125537  2.273688\n 1.6221616  1.3260311  0.658585   1.8775612  1.8313575  2.2752862\n 1.6645875  1.8122795  1.113964   1.9115771  1.2668967  1.3053045\n 1.489118   1.1185968  1.7912353  1.5426129  1.5293145  1.0852541\n 0.7953611  0.56654686 1.4592907  1.3322864  1.1731029  1.2318578\n 1.112714   1.0739491 ]\nEpoch 4, Loss: [1.3894317  1.1964879  2.1229553  1.2457461  1.0841359  0.90720505\n 1.4587579  1.6796124  1.6038694  1.7473117  0.90633166 1.0504787\n 2.1833932  1.585569   1.9138715  1.5565783  1.4608314  2.1355772\n 1.325706   1.1674374  1.1099633  0.32460663 2.231346   1.8727543\n 1.3649215  0.530485   1.1195354  0.59480566 2.7434623  2.1069894\n 1.0952104  1.1922302 ]\nEpoch 4, Loss: [0.6195767  2.1987915  1.7695197  1.0718542  1.858121   1.2570965\n 2.0146592  1.0847292  1.1792467  1.7134258  1.1316923  1.5233529\n 2.6232932  1.2834259  2.006963   1.4316208  1.5474659  1.6083134\n 1.4274828  0.32707104 1.6606675  1.4333023  1.1413113  1.7495818\n 1.554426   1.4465393  1.2719885  1.8959035  1.716143   0.998711\n 2.007231   1.3534176 ]\nEpoch 4, Loss: [1.7661704  1.5395226  1.1096531  1.2949023  1.1363764  0.85505265\n 0.73689187 1.1323339  1.2061276  1.4607697  1.6230744  1.3744124\n 1.1042893  0.9659949  1.2170138  0.7791343  1.7068037  1.3862232\n 1.0518792  1.2319392  1.4123827  1.1422683  0.5142945  1.3575208\n 1.2413824  2.011089   1.3604953  0.9811952  1.9185727  1.7564298\n 1.4142143  1.0119644 ]\nEpoch 4, Loss: [0.75395745 1.4238713  0.6214746  1.0511136  2.0570025  1.695946\n 0.5207561  1.9105628  1.269362   1.5575073  1.3591481  1.5165211\n 1.0744308  2.2150667  1.3388852  1.6460314  1.2521136  0.83123326\n 0.8586156  1.4173934  1.3245256  2.7318912  1.3007969  2.0638368\n 0.9363942  1.1506362  0.82539916 1.6532239  1.4049894  1.8276575\n 1.5950794  0.68217057]\nEpoch 4, Loss: [1.0949728  0.89947844 0.5639484  2.915938   1.0537486  0.87503576\n 1.6165566  0.8140265  1.5293775  1.7111608  1.7977477  2.2270303\n 1.2056417  1.9752922  0.68928456 2.025453   1.7279141  1.8796964\n 0.80039483 1.3967065  2.1069415  1.1477315  2.049131   1.2841151\n 1.8452928  1.5415112  0.51333255 1.0590373  1.6590176  0.88975716\n 0.65405345 1.770392  ]\nEpoch 4, Loss: [1.4990841  1.1346903  1.0550737  1.2625221  1.5893719  1.7375964\n 1.0728894  1.1015738  2.6081986  0.77501655 0.9598521  2.242732\n 1.3782965  1.0802513  1.3896476  2.2700906  1.8040855  1.6665738\n 1.427122   1.8263429  1.3030654  1.5332469  1.5406749  1.8183436\n 1.9551147  1.1947701  1.2319171  1.9909797  1.4346428  1.7469678\n 0.9219419  1.6901244 ]\nEpoch 4, Loss: [0.9291708  1.0700158  1.1750544  0.98044425 1.0470711  0.73137605\n 1.1366711  1.5887369  0.8631261  1.3374934  0.8144508  1.3798852\n 1.3567754  1.4216597  1.0478467  1.5707835  0.80654883 1.3127044\n 1.6423558  1.517226   1.6969179  0.4029975  1.7963512  0.9250758\n 2.4782512  1.0847008  1.1373101  1.7928346  1.3193915  2.0142663\n 1.2078713  1.6731256 ]\nEpoch 4, Loss: [2.0326693  0.88932616 1.0313689  1.9369268  0.90987086 2.3867576\n 1.9316325  2.592202   0.8593778  1.368273   2.022706   1.3620933\n 2.8979313  1.5036683  0.79894894 1.1063279  1.7304865  1.134604\n 1.4596348  1.0632491  1.327374   1.0534068  0.44766995 0.9696888\n 1.5428703  0.98978114 1.7674212  1.6518084  1.0674273  1.415696\n 1.2670921  1.8606805 ]\nEpoch 4, Loss: [2.1925154  2.2154593  2.231067   3.0484018  1.1846285  0.6896936\n 2.001664   1.6623983  0.92500037 1.3981698  0.746824   1.4064827\n 1.101163   0.95113564 1.3432319  1.7576752  1.1986879  1.337019\n 1.8455433  0.8262578  1.4514792  1.2396128  0.7073983  1.5669931\n 1.361459   1.1037253  1.0685025  1.2307451  1.0586642  2.0417545\n 2.4833686  1.4097449 ]\nEpoch 4, Loss: [2.0914457  0.6113469  1.3691196  1.0615456  1.2625844  2.2211728\n 0.67576766 1.372297   2.7920394  0.69297045 2.9153473  1.7874503\n 1.9086995  1.3131164  1.6951535  1.1802033  1.4909469  1.0738313\n 1.5360221  0.87320787 1.1833851  1.5289177  3.548335   2.1956332\n 1.1023903  1.4476525  1.7200603  0.80812377 1.6395918  1.7063295\n 0.5329111  1.5512176 ]\nEpoch 4, Loss: [0.8873367  1.6607693  1.9251951  1.0654855  2.0958548  1.634582\n 0.606001   1.2857895  1.308543   2.4538577  0.9276307  0.8528535\n 1.6533653  0.43201783 0.9698413  2.3152602  1.7100824  0.6775677\n 0.5780788  2.0178185  1.4244887  1.1133311  1.6166221  1.0511512\n 1.1650138  0.9776762  1.4751683  1.930705   2.423734   2.1139162\n 1.5794833  2.2853575 ]\nEpoch 4, Loss: [0.78323    1.4658594  1.9285886  1.8267956  0.74223197 0.66721624\n 2.694306   1.0397692  1.5357823  1.7777584  1.9974363  1.4236735\n 1.1459128  1.415686   1.4652629  0.6481495  0.8587357  1.3607808\n 2.4200993  1.320952   0.6415587  1.1477283  1.3737487  1.4689204\n 1.3687948  0.6972406  1.4137874  0.6518384  1.2002499  0.5299189\n 1.184309   0.83655244]\nEpoch 4, Loss: [1.4800122  1.7095093  1.4976302  1.2948779  1.743725   1.3431364\n 1.764985   2.0094032  0.95872045 1.3137826  1.0029     1.6045465\n 1.3116747  0.91576546 1.837361   0.6549875  1.5399593  1.8498183\n 1.1242868  1.2218248  1.2468835  1.1229801  2.507502   2.5124521\n 2.0944297  0.8742048  1.75689    0.23443176 1.949546   2.0671844\n 1.018046   0.98126453]\nEpoch 4, Loss: [1.5883167  0.5821796  1.0719225  1.2080379  0.76298374 0.25018615\n 1.7545404  1.457934   2.204828   1.2889657  0.9387489  1.9802704\n 1.5943346  0.60612303 0.8353885  1.7051522  2.6372445  2.1255608\n 2.0700538  0.6266152  1.3450253  1.5180936  1.6393594  0.94237655\n 1.6697836  1.4013193  1.6632024  2.076348   2.4262922  0.72617793\n 1.8403373  1.8619812 ]\nEpoch 4, Loss: [2.6202676  1.0107199  2.2093973  1.4380499  1.735576   1.2033631\n 1.4146957  1.773234   1.3838248  1.8702604  0.91558886 2.4264565\n 3.7861297  1.5464885  1.3687142  2.1984828  1.0575303  1.2672249\n 1.2453629  1.4853817  1.108118   2.1339235  1.915574   1.8086689\n 0.9225855  1.8221761  1.5977935  1.0882285  0.7864418  1.3107666\n 0.97200084 1.0523773 ]\nEpoch 4, Loss: [2.7007997  1.2328217  1.4297749  1.6035017  0.8060115  1.4767406\n 1.0842419  1.622803   1.5023263  1.8049065  1.6643864  1.8261517\n 1.2585655  1.7757015  1.7042485  1.0502015  0.6794515  1.6617453\n 1.6214932  2.0007205  1.4361904  1.4357904  1.8037207  2.412965\n 1.736316   0.6578603  0.8512034  1.2295825  1.6041334  0.9100365\n 0.52681345 0.5239942 ]\nEpoch 4, Loss: [1.7838163  1.9449549  1.9029682  1.1997267  1.8542609  1.0337088\n 0.9778744  2.391177   1.2502371  0.57397217 2.0011852  0.8797152\n 1.5127711  1.3019707  1.1195117  1.3979255  1.4542962  1.0374304\n 1.5844969  2.1998417  2.602196   1.4965137  0.9888849  0.7339232\n 2.1917133  1.2009001  0.49320373 0.98482764 1.3040341  0.75289834\n 0.73413587 1.4072583 ]\nEpoch 4, Loss: [1.2144674  1.4998171  1.2870156  1.4721789  1.762689   1.6124654\n 1.8425832  1.5935673  1.4665904  2.035517   1.6719681  1.6468174\n 1.5584846  1.6292073  1.3867373  1.2203873  2.5142393  1.1314609\n 1.4366226  1.1490403  0.68060267 1.5875411  1.2596189  1.0472524\n 1.1025711  1.9538523  2.280674   2.0091422  1.7361821  1.6069258\n 1.7273301  2.5901597 ]\nEpoch 4, Loss: [0.6387048  1.2352217  1.9083372  2.4319484  0.5747781  1.9675217\n 1.0095587  0.9787365  1.276973   1.3680658  0.76834404 1.9736948\n 0.54099435 1.5555885  1.1028119  2.0880883  1.3614838  1.1729668\n 1.9803178  1.7201518  2.0667772  0.96587676 1.8098366  0.5819414\n 0.7918851  1.6636062  1.5987083  0.77705115 0.8822214  1.9416997\n 2.0915527  1.1405493 ]\nEpoch 4, Loss: [1.2863029  1.3086644  1.638982   1.8036509  1.4029458  1.2033463\n 2.6559324  1.2783552  0.9237681  2.3147652  2.162904   1.814383\n 2.3427966  1.305225   1.0653982  0.8446125  2.2161283  1.2936432\n 1.7845584  1.259892   1.6371989  1.4331683  1.291048   0.9019741\n 1.4134154  0.7169962  1.5063014  0.89499    0.41177526 0.7634124\n 1.0501845  0.5879948 ]\nEpoch 4, Loss: [1.1154977  1.7433946  0.9732598  1.2703803  0.720638   1.648957\n 1.5104386  1.2995185  1.2654285  2.2009447  1.4930937  1.4367036\n 1.0643188  1.1148167  0.89994204 1.1824983  0.98521805 1.323533\n 0.6891803  1.9099865  1.6517226  1.8350034  1.8827109  1.3305526\n 0.7533248  1.4888554  1.20523    2.2463298  1.5648627  1.5971506\n 0.8952551  1.6547095 ]\nEpoch 4, Loss: [1.2253358  1.3202957  1.0599653  0.22813217 0.5819173  1.4121433\n 1.0432363  1.5377363  1.093845   1.1146595  0.65880084 2.2052908\n 1.6331186  2.3043907  1.8087504  1.3528792  1.8529452  1.2732778\n 1.0985606  2.51551    1.4958237  0.8315257  0.4732722  2.3507018\n 1.7209532  1.9536085  1.0050911  1.332875   0.7009266  0.9751542\n 1.2294601  1.7235775 ]\nEpoch 4, Loss: [2.2023785  1.3730485  2.8090653  1.7249864  1.2508363  1.1576649\n 1.046383   0.79518497 2.1143615  1.6376522  0.73773605 0.9568156\n 2.98733    1.3562464  1.0225064  1.728703   1.9817411  1.120114\n 0.7555564  1.8502256  0.806003   2.5374002  1.0860933  2.0506675\n 0.5112776  0.9799055  0.5370037  1.4248276  1.1682756  1.7005572\n 1.6465763  1.236895  ]\nEpoch 4, Loss: [1.0085791  2.2784514  1.4943018  2.0601792  1.511891   1.6335796\n 1.1414425  1.7574341  0.32138062 1.5634778  1.2512633  0.6287894\n 1.2501916  1.8781432  1.7055473  1.6743822  1.286919   1.2745306\n 0.9394036  0.9399774  1.5785718  1.1335205  1.3855453  1.0622482\n 1.6295135  2.1757193  0.8967021  2.5201626  1.4692519  1.6811769\n 1.2893535  1.3376402 ]\nEpoch 4, Loss: [1.2312171  1.9758301  2.7463071  1.0458694  1.3961942  0.70540625\n 0.8839365  0.7125738  2.0080357  1.5463101  1.3901922  1.7759147\n 1.568739   1.7618393  1.4594225  1.2917186  1.3868778  1.5297848\n 2.566163   0.9748838  1.0979319  1.871925   2.59992    0.5687153\n 1.619842   1.147122   2.3343716  1.6448222  2.2587717  2.0765948\n 0.8695069  2.7587695 ]\nEpoch 4, Loss: [1.7950791  1.4892558  1.4831889  1.3947626  1.0139561  0.85916615\n 1.3622364  0.93161094 1.1315421  0.9515188  1.021543   1.1995592\n 1.8322334  0.4968642  1.5934873  2.0994267  2.0037053  1.3151867\n 1.332507   1.1767156  0.7723438  0.7802739  1.9422482  0.93104005\n 1.2263317  1.4695717  1.0864463  1.1923299  1.5790098  0.94571865\n 1.7999302  2.0112627 ]\nEpoch 4, Loss: [1.4589192  0.78367716 1.6355925  1.288961   1.0087955  0.6295971\n 0.67637867 0.98722535 2.2833996  1.1084203  0.8001405  1.3399694\n 1.0321258  2.0053952  1.5535941  1.2217896  1.8149412  2.209457\n 1.2918485  1.3533387  0.4923132  0.8807133  1.9564713  1.2387003\n 1.4371674  1.6197551  1.8800508  1.0159913  1.275359   1.7841463\n 1.5904248  1.1851673 ]\nEpoch 4, Loss: [1.878883   1.5453826  0.93773043 1.5385953  1.9620664  0.6323219\n 1.3397528  1.0792389  1.2139629  1.6954045  0.87099546 0.82578874\n 0.70254385 1.3628013  1.6095709  1.2572103  1.0038517  0.4074046\n 2.3955717  1.159554   2.2852244  1.6877365  1.9241999  1.516067\n 2.3516402  2.0225475  1.3166281  1.3025955  2.1819847  1.4384731\n 1.363806   1.0757923 ]\nEpoch 4, Loss: [1.8439294  1.1264397  0.84932035 1.2313305  1.0033348  0.61983806\n 1.5174965  1.6799643  0.709779   1.0949004  2.5183651  1.0230786\n 1.4082214  1.1769774  0.5372301  0.8308154  1.225175   1.520708\n 0.8243413  2.3366778  1.5064481  0.9961341  1.1688775  0.4878346\n 1.8481619  1.5309188  2.200854   1.2585115  1.7078001  1.047776\n 1.6643792  0.8782007 ]\nEpoch 4, Loss: [1.0639691  2.623148   2.2777164  1.3691237  0.9855289  1.9797782\n 1.75173    1.0966665  1.7106284  0.8876381  1.6508417  1.5671546\n 1.835988   0.36517534 0.94579214 2.2677877  2.8730822  1.5048687\n 1.5777384  1.9836506  2.3864915  0.800509   1.2320033  2.415748\n 1.2602603  2.1026416  1.1584331  1.8724711  0.6521712  0.94729894\n 2.1725829  1.4902344 ]\nEpoch 4, Loss: [1.9600189  0.8189684  0.67591256 2.4856105  1.6587754  0.94739485\n 0.57310575 1.8022113 ]\nTest Accuracy: 8.00%\n","output_type":"stream"}]}]}